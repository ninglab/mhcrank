{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, norm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv('./ap_benchmarking_template.csv.gz')\n",
    "bench_len = bench.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./ap_models3/selected_models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create ensembles and add to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold0_1 = pd.read_csv('')\n",
    "fold0_2 = pd.Series.from_csv('')\n",
    "\n",
    "fold1_1 = \n",
    "fold1_2 = \n",
    "\n",
    "fold2_1 = \n",
    "fold2_2 = \n",
    "\n",
    "fold3_1 = \n",
    "fold3_2 = \n",
    "\n",
    "top1_1 = \n",
    "top1_2 = \n",
    "top1_3 = \n",
    "top1_4 = \n",
    "\n",
    "top2_1 = \n",
    "top2_2 = \n",
    "top2_3 = \n",
    "top2_4 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define Ensembles\n",
    "\n",
    "complete_ensemble = []\n",
    "ensemble_folds = []\n",
    "ensemble_tops = []\n",
    "ensemble_fold_best = []\n",
    "ensemble_top_best = []\n",
    "ensemble_foldtop_best = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add Ensembles to models \n",
    "\n",
    "bench['complete_ensemble'] = complete_ensemble\n",
    "bench['ensemble_folds'] = ensemble_folds\n",
    "bench['ensemble_tops'] = ensemble_tops\n",
    "bench['ensemble_fold_best'] = ensemble_fold_best\n",
    "bench['ensemble_top_best'] = ensemble_top_best\n",
    "bench['ensemble_foldtop_best'] = ensemble_foldtop_best\n",
    "\n",
    "models = [\"netmhcpan4.el\", \"mhcflurry2.ap.with_flanks\", 'complete_ensemble', 'ensemble_folds', \n",
    "          'ensemble_tops', 'ensemble_fold_best', 'ensemble_top_best', 'ensemble_foldtop_best']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define stat functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPPV(data, k):\n",
    "    name = data.columns[1]\n",
    "    sort = data.sort_values(by=name)\n",
    "    avg = sort.iloc[:k, 'hit'].mean()\n",
    "    return ppv\n",
    "    \n",
    "    \n",
    "def getNDCG(data, k):\n",
    "    name = data.columns[1]\n",
    "    true = data.loc[:, 'hit']\n",
    "    pred = data.loc[:, name]\n",
    "    ndcg = ndcg_score(true, pred, k=k)\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def bootstrap(bench, ppv_values, ndcg_values, sample_size, iterations, models, k_vals):\n",
    "    for i in range(iterations):\n",
    "        if i % 500 == 0:\n",
    "            print(\"Iteration: \" + str(i))\n",
    "            \n",
    "        sample = resample(df, n_samples=sample_size)\n",
    "        for k in k_vals:\n",
    "            for model in models:\n",
    "                data = sample.loc[:, ['hit', model]]\n",
    "                ppv_values[model + '_' + str(k)].append(get_PPV(data, k))\n",
    "                ndcg_values[model + '_' + str(k)].append(get_NDCG(data, k))\n",
    "                \n",
    "    return ppv_values, ndcg_values\n",
    "\n",
    "def fill_df(values_dict, df, comp1, comp2):\n",
    "    for key in values_dict.keys()\n",
    "        meta = key.split('_')\n",
    "        k = int(meta[-1])\n",
    "        mod_name = meta[:-1]\n",
    "        mod_avg = mod_name + '_mean'\n",
    "        mod_ci = mod_name + '_95CI'\n",
    "        mod_p1 = mod_name + '_pvalue_pan'\n",
    "        mod_p2 = mod_name + '_pvalue_flurry'\n",
    "        \n",
    "        data = np.asarray(values_dict[key])\n",
    "        compare1 = np.asarray(values_dict[comp1 + '_' + str(k)])\n",
    "        compare2 = np.asarray(values_dict[comp2 + '_' + str(k)])\n",
    "        \n",
    "        avg, sigma = np.mean(data), np.std(data)\n",
    "        ci = norm.interval(0.95, loc=avg, scale=sigma/sqrt(N))[0]\n",
    "        \n",
    "        p1 = ttest_ind(data, compare1).pvalue\n",
    "        p2 = ttest_ind(data, compare1).pvalue\n",
    "        \n",
    "        df.loc[k, mod_avg] = avg\n",
    "        df.loc[k, mod_ci] = avg - ci\n",
    "        df.loc[k, mod_p1] = p1\n",
    "        df.loc[k, mod_p2] = p2\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"netmhcpan4.el\", \"mhcflurry2.ap.with_flanks\", 'complete_ensemble', 'ensemble_folds', \n",
    "          'ensemble_tops', 'ensemble_fold_best', 'ensemble_top_best', 'ensemble_foldtop_best']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make empty dfs and define k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [10, 25, 50, 100, 250, 500]\n",
    "n_samples = 100000\n",
    "iters = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_total = np.zeros((len(k), 8))\n",
    "ppv = pd.DataFrame(zeros_total, columns=models)\n",
    "ndcg = pd.DataFrame(zeros_total, columns=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_boot = np.zeros((len(k), 32))\n",
    "\n",
    "avg_mods = [model + '_mean' for model in models]\n",
    "ci_mods = [model + '_95CI' for model in models]\n",
    "pval1_mods = [model + '_pvalue_pan' for model in models]\n",
    "pval2_mods = [model + '_pvalue_flurry' for model in models]\n",
    "\n",
    "boot_models = []\n",
    "for i in range(len(avg_mods)):\n",
    "    boot_models.append(avg_mods[i])\n",
    "    boot_models.append(ci_mods[i])\n",
    "    boot_models.append(pval1_mods[i])\n",
    "    boot_models.append(pval2_mods[i])\n",
    "\n",
    "boot_ppv = pd.DataFrame(zeros_boot, columns=boot_models)\n",
    "boot_ndcg = pd.DataFrame(zeros_boot, columns=boot_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  make output dict for bootstrapping\n",
    "\n",
    "ppv_values = dict()\n",
    "ndcg_values = dict()\n",
    "for i in k:\n",
    "    for model in models:\n",
    "        ppv_values[model + '_' + str(i)] = list()\n",
    "        ndcg_values[model + '_' + str(i)] = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and NDCG at k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculate for entire dataset\n",
    "\n",
    "for i in k:\n",
    "    for model in models:\n",
    "        data = bench.loc[:, ['hit', model]]\n",
    "        ppv.loc[i, model] = getPPV(data, k)\n",
    "        ndcg.loc[i, model] = getNDCG(data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Bootstrap Resampling\n",
    "\n",
    "ppv_values, ndcg_values = bootstrap(bench, ppv_values, ndcg_values, n_samples, iters, models, k_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calc Mean, 95% CI, and p_value (comapred to mhcflurry2.0 AP)\n",
    "\n",
    "boot_ppv = fill_df(ppv_values, boot_ppv, \"netmhcpan4.el\", \"mhcflurry2.ap.with_flanks\")\n",
    "boot_ndcg = fill_df(ndcg_values, boot_ndcg,\"netmhcpan4.el\", \"mhcflurry2.ap.with_flanks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to excel file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('mhc_rank_benchmarking_rnd3.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "boot_ppv.to_excel(writer, sheet_name='bootstrapped_precision_at_k')\n",
    "ppv.to_excel(writer, sheet_name='total_precision_at_k')\n",
    "\n",
    "boot_ndcg.to_excel(writer, sheet_name='bootstrapped_ndcg_at_k')\n",
    "ndcg.to_excel(writer, sheet_name='total_ndcg_at_k')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
